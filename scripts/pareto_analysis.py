# This script analyzes the raw output generated by java_analysis.py and generates another csv containing
# classifying each repository into categories of pareto analysis (10_90, 20_80, ...)
# 10_90 means 10% of components contains 90% of arch smells.

import os
import glob
import os
import re
from collections import Counter

import pandas as pd
from pandas.errors import EmptyDataError

### Read csv file and create pandas DataFrame
def read_file(filepath, column_list, duplicate_flag=True):
    if os.path.exists(filepath):
        try:
            data_frame = pd.read_csv(filepath, usecols=column_list)
            if not data_frame.empty:
                if duplicate_flag:
                    return data_frame
                else:
                    return data_frame.drop_duplicates()
            else:
                return None
        except EmptyDataError:
            return None
    else:
        return None

# Project Name, Package Name, Architecture Smell, Cause of the Smell
def _get_total_smells(dj_result_folder, project, project_smell_data):
    cur_arch_file = os.path.join(dj_result_folder, project, r'ArchitectureSmells.csv')
    # Count cyclic dep from designite results
    cyclic_dep_count = 0
    if os.path.exists(cur_arch_file):
        with open(cur_arch_file, 'r', encoding='UTF-8', errors='ignore') as reader:
            for line in reader.readlines():
                tokens = line.split(',')
                if 'test' in str(tokens[1]) or 'sample' in str(tokens[1]):
                    continue
                if len(tokens) > 2:
                    if tokens[2] == 'Cyclic Dependency':
                        cyclic_dep_count += 1
    total_smells = project_smell_data['Unstable Dependency'].sum() + project_smell_data['Ambiguous Interface'].sum() + \
                   project_smell_data['God Component'].sum() + project_smell_data['Feature Concentration'].sum() + \
                   project_smell_data['Scattered Functionality'].sum() + project_smell_data['Dense Structure'].sum() + \
                   cyclic_dep_count
    return total_smells

# Architecture smell, Project, Namespace, Cause, Responsible Classes, Participating Classes
def _get_total_smells_cs(dj_result_folder, project, project_smell_data):
    total_smells = 0
    cyclic_dep_count = 0
    for file in os.listdir(os.path.join(dj_result_folder, project)):
        if not file.endswith('_ArchSmells.csv'):
            continue
        cur_arch_file = os.path.join(dj_result_folder, project, file)
        # Count cyclic dep from designite results
        # Architecture smell, Project, Namespace, Cause, Responsible Classes, Participating Classes
        if os.path.exists(cur_arch_file):
            with open(cur_arch_file, 'r', encoding='UTF-8', errors='ignore') as reader:
                for line in reader.readlines():
                    tokens = line.split(',')
                    if 'test' in str(tokens[2]) or 'sample' in str(tokens[2]):
                        continue
                    if len(tokens) > 2:
                        if tokens[0] == 'Cyclic Dependency':
                            cyclic_dep_count += 1
    total_smells += project_smell_data['Unstable Dependency'].sum() + project_smell_data['Ambiguous Interface'].sum() + \
                    project_smell_data['God Component'].sum() + project_smell_data['Feature Concentration'].sum() + \
                    project_smell_data['Scattered Functionality'].sum() + project_smell_data[
                        'Dense Structure'].sum() + cyclic_dep_count
    return total_smells


def _get_other_component_in_cycles(cycles_in_project, pkg):
    component_list = []
    to_remove = []
    for cycle_list in cycles_in_project:
        if pkg in cycle_list:
            for item in cycle_list:
                if not item == pkg:
                    component_list.append(item)
            to_remove.append(cycle_list)
    for item in to_remove:
        cycles_in_project.remove(item)
    return component_list


def _deduct_cycle_components(project_smell_data, component_cycles, pkg):
    is_changed = False
    for item, row in project_smell_data.iterrows():
        for component in component_cycles:
            if row[pkg] == component:
                cur_cd = int(project_smell_data.at[item, 'Cyclic Dependency'])
                if cur_cd > 0:
                    is_changed = True
                    project_smell_data.at[item, 'Cyclic Dependency'] = cur_cd - 1
                    project_smell_data.at[item, 'Total Architecture Smells'] = int(project_smell_data.at[item, 'Total Architecture Smells']) - 1
    return is_changed


def _get_smells_for_category(component_percent, project_smell_data, total_packages, cycles_in_project, pkg_name):
    components_to_include = int(round(float(total_packages * component_percent) / float(100)))
    smell_count = 0
    is_order_changed = True
    project_smell_data = project_smell_data.sort_values(by=['Total Architecture Smells'], ascending=False)
    while is_order_changed:
        counter = 0
        smell_count = 0
        for item, row in project_smell_data.iterrows():
            if counter >= components_to_include:
                is_order_changed = False
                break
            # We need to figure out how many cyclic dep smells are present in this component and
            # need to detect cyclic dep from other components that are belonging to already counted cyclic dep
            component_cycles = _get_other_component_in_cycles(cycles_in_project, row[pkg_name])
            is_order_changed = _deduct_cycle_components(project_smell_data, component_cycles, pkg_name)
            smell_count += row['Total Architecture Smells']
            counter += 1
            if is_order_changed:
                break
        if is_order_changed: # whenever the order is changed, we need to sort and repeat the process
            project_smell_data = project_smell_data.sort_values(by=['Total Architecture Smells'], ascending=False)

    return smell_count


def _get_cycles(cyclic_data_file, project):
    all_project_lines = []
    with open(cyclic_data_file, 'r') as file:
        for line in file.readlines():
            tokens = line.split(',')
            if len(tokens) > 2:
                if tokens[1] == project:
                    all_project_lines.append(line)
    project_cycles = []
    for cycle_length in range(2, 6):
        for line in all_project_lines:
            tokens = line.split(',')
            if len(tokens) > 3:
                if int(tokens[3]) == cycle_length:
                    cur_pkg_list = list(tokens[2].split(' '))
                    project_cycles.append(cur_pkg_list)
    return project_cycles

def aggregation_function(x):
    d = {}
    d['Count namespaces'] = x['Combination'].nunique()
    d['Sum LOC'] = x['LOC'].sum()
    d['Average LOC'] = x['LOC'].mean()

    return pd.Series(d, index=['Count namespaces', 'Sum LOC', 'Average LOC'])

### Parse repo to calculate metrics
def calculate_cs_loc(repositoryPath, projectList):
    namespaceList = []
    metricsList = []

    repo_loc = 0

    for project in projectList:
        if re.search('test|sample', project, re.IGNORECASE):
            print('Project: ' + project + ' is excluded from analysis')
        else:
            print(project)

            projectPath = os.path.join(repositoryPath, project)
            class_metrics_raw = read_file(projectPath + r'_ClassMetrics.csv', ['Namespace','Type','LOC'], False)

            if class_metrics_raw is not None:
                class_metrics_raw['Namespace'] = class_metrics_raw['Namespace'].fillna('Empty.Namespace')
                class_metrics_raw = class_metrics_raw[class_metrics_raw['Namespace'].str.contains('test|sample', flags=re.IGNORECASE, regex=True) == False]

                project_loc = class_metrics_raw['LOC'].sum()
                repo_loc += project_loc

    return repo_loc

def analyze_pareto(raw_filepath, dj_result_folder, output_filepath, cyclic_data_file, lang='java'):
    if lang == 'java':
        prj_name = 'Project Name'
        pkg_name = 'Package Name'
    else:
        prj_name = 'Repository'
        pkg_name = 'Namespace'

    column_list = [prj_name, pkg_name,
                   'Cyclic Dependency', 'Unstable Dependency', 'Ambiguous Interface',
                   'God Component', 'Feature Concentration', 'Scattered Functionality',
                   'Dense Structure', 'Total Architecture Smells']
    smell_data = read_file(raw_filepath, column_list)
    with open(output_filepath, "w") as writer:
        writer.write('Project,Pareto category\n')
        for project in smell_data.groupby(prj_name):
            print('processing ' + project[0])
            project_smell_data = project[1].sort_values(by=['Total Architecture Smells'], ascending=False)
            total_packages, cols = project[1].shape
            if lang == 'java':
                total_smells = _get_total_smells(dj_result_folder, project[0], project_smell_data)
            else:
                total_smells = _get_total_smells_cs(dj_result_folder, project[0], project_smell_data)

            project_category = -1
            if total_smells == 0:
                project_category = 100
            else:
                for category in range(10, 100, 10):
                    cycles_in_project = _get_cycles(cyclic_data_file, project[0])
                    cur_smell_count = _get_smells_for_category(category, project_smell_data, total_packages,
                                                               cycles_in_project, pkg_name)
                    smell_percent = float(cur_smell_count * 100) / float(total_smells)
                    smell_percent_pareto = 100 - category
                    if smell_percent >= smell_percent_pareto:
                        project_category = category
                        break


            # Filter projects that have LOC < 1K
            if lang == 'java':
                projectMetrics = read_file(os.path.join(dj_result_folder, project[0], r'TypeMetrics.csv'),
                                        ['Project Name', 'Package Name', 'LOC'], True)


                if projectMetrics is not None:
                    projectMetrics = projectMetrics[
                        projectMetrics['Package Name'].str.contains('test|sample', flags=re.IGNORECASE,
                                                                    regex=True) == False]
                    project_loc = projectMetrics['LOC'].sum()
                    print(project_loc)
                    if project_loc > 1000:
                        writer.write(project[0] + ',' + str(project_category) + '\n')
                    else:
                        print('Project: ' + project[0] + ' has less than 1K LOC.')

            elif lang == 'cs':
                folder = os.path.join(dj_result_folder, project[0])
                projectList = []
                for file in os.listdir(folder):
                    if file.endswith('_NamespaceMetrics.csv'):
                        prj = file.rsplit('_', 1)[0]
                        if prj not in projectList:
                            projectList.append(prj)

                if len(projectList) != 0:
                    total_loc = calculate_cs_loc(folder, projectList)
                    if total_loc > 1000:
                        writer.write(project[0] + ',' + str(project_category) + '\n')
                    else:
                        print('Project: ' + project[0] + ' has less than 1K LOC.')


if __name__ == "__main__":
    # analyze_pareto(raw_filepath=r'path/data/results/raw_java.csv',
    #                dj_result_folder=r'path/data/designitejava_out',
    #                output_filepath=r'path/data/results/pareto_java.csv',
    #                cyclic_data_file=r'path/data/results/cyclicFile.csv')
    analyze_pareto(raw_filepath=r'path/data/results/raw_cs.csv',
                   dj_result_folder=r'path/data/designite_out',
                   output_filepath=r'path/data/results/pareto_cs.csv',
                   cyclic_data_file=r'path/data/results/cyclicFile_cs.csv',
                   lang='cs')
